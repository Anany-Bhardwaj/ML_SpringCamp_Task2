{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "                                   BONUS TASK\n",
    "This code uses the YOLOv3 object detection model (via OpenCV's DNN module) to count the maximum number of people detected in any frame of a given video. It defines a function load_yolo_model that loads the YOLOv3 configuration, weights, and COCO class names from specified file paths. It also extracts the output layer names required for inference.\n",
    "Finally it runs the count_people_in_video function on a sample video and prints the estimated maximum number of people detected in a brawl."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e03f91d82a6f9b5d"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-25T15:45:41.701733Z",
     "start_time": "2025-03-25T15:44:39.918302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 153 frames. Maximum smoothed fighting people count in a frame: 6\n",
      "Estimated maximum number of people involved in the brawl (smoothed): 6\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "def load_yolo_model(cfg_path=r\"C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\darknet\\cfg\\yolov3.cfg\", \n",
    "                    weights_path=r\"C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\yolov3.weights\", \n",
    "                    names_path=r\"C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\darknet\\data\\coco.names\"):\n",
    "    net = cv2.dnn.readNetFromDarknet(cfg_path, weights_path)\n",
    "    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "    \n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers().flatten()]\n",
    "    \n",
    "    with open(names_path, \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    return net, output_layers, classes\n",
    "\n",
    "def compute_iou(boxA, boxB):\n",
    "    \"\"\"Compute Intersection over Union (IoU) between two bounding boxes.\"\"\"\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
    "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
    "    \n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = boxA[2] * boxA[3]\n",
    "    boxBArea = boxB[2] * boxB[3]\n",
    "    \n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
    "    return iou\n",
    "\n",
    "def detect_fight_in_frame(boxes, distance_threshold=60, iou_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Given a list of bounding boxes, determine which persons are close enough (or overlapping)\n",
    "    to be flagged as potentially fighting.\n",
    "    \n",
    "    The function returns a set of indices for persons meeting at least one of the conditions.\n",
    "    \"\"\"\n",
    "    fight_indices = set()\n",
    "    centers = []\n",
    "    \n",
    "    for (x, y, w, h) in boxes:\n",
    "        center = (x + w / 2, y + h / 2)\n",
    "        centers.append(center)\n",
    "    \n",
    "    # Compare each pair of boxes\n",
    "    for i in range(len(boxes)):\n",
    "        for j in range(i + 1, len(boxes)):\n",
    "            # Compute Euclidean distance between centers\n",
    "            dist = np.linalg.norm(np.array(centers[i]) - np.array(centers[j]))\n",
    "            # Compute IoU for overlap between bounding boxes\n",
    "            iou = compute_iou(boxes[i], boxes[j])\n",
    "            # Flag if they are either very close or overlapping significantly\n",
    "            if dist < distance_threshold or iou > iou_threshold:\n",
    "                fight_indices.add(i)\n",
    "                fight_indices.add(j)\n",
    "                \n",
    "    return fight_indices\n",
    "\n",
    "def count_fighting_people_in_video(video_path, conf_threshold=0.5, nms_threshold=0.4, \n",
    "                                   distance_threshold=60, iou_threshold=0.1, smoothing_window=5):\n",
    "    \"\"\"\n",
    "    Process a video, detect persons in each frame, and use heuristics to determine\n",
    "    which persons are potentially fighting. Temporal smoothing is applied to reduce\n",
    "    fluctuations in the fighting count. Returns the maximum (smoothed) count of people\n",
    "    detected as involved in a fight in any frame.\n",
    "    \"\"\"\n",
    "    # Load YOLO model\n",
    "    net, output_layers, classes = load_yolo_model()\n",
    "    \n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    max_fight_count = 0\n",
    "    frame_count = 0\n",
    "\n",
    "    # Queue for smoothing fighting count over a few frames\n",
    "    fight_count_queue = deque(maxlen=smoothing_window)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        height, width = frame.shape[:2]\n",
    "        \n",
    "        # Create a blob from the image\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        outs = net.forward(output_layers)\n",
    "        \n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        class_ids = []\n",
    "\n",
    "        # Process YOLO outputs\n",
    "        for out in outs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                \n",
    "                if confidence > conf_threshold and classes[class_id] == \"person\":\n",
    "                    center_x, center_y, w, h = (detection[0:4] * np.array([width, height, width, height])).astype(\"int\")\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "                    \n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "        \n",
    "        # Apply Non-Maximum Suppression\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "        final_boxes = [boxes[i] for i in indices.flatten()] if indices is not None else []\n",
    "\n",
    "        # Use heuristics to determine fighting individuals\n",
    "        fighting_indices = detect_fight_in_frame(final_boxes, distance_threshold, iou_threshold)\n",
    "        fight_count = len(fighting_indices)\n",
    "        \n",
    "        # Add the current fighting count to the smoothing queue\n",
    "        fight_count_queue.append(fight_count)\n",
    "        # Compute the median fighting count over the smoothing window\n",
    "        smoothed_fight_count = int(np.median(list(fight_count_queue)))\n",
    "        \n",
    "        if smoothed_fight_count > max_fight_count:\n",
    "            max_fight_count = smoothed_fight_count\n",
    "\n",
    "        # (Optional) Draw bounding boxes and annotate potential fighting persons\n",
    "        for idx, box in enumerate(final_boxes):\n",
    "            x, y, w, h = box\n",
    "            color = (0, 0, 255) if idx in fighting_indices else (0, 255, 0)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "        \n",
    "        # Uncomment the following lines to display frames during processing:\n",
    "        # cv2.imshow(\"Frame\", frame)\n",
    "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        #     break\n",
    "\n",
    "    cap.release()\n",
    "    # cv2.destroyAllWindows()\n",
    "    print(f\"Processed {frame_count} frames. Maximum smoothed fighting people count in a frame: {max_fight_count}\")\n",
    "    return max_fight_count\n",
    "\n",
    "# Example usage:\n",
    "video_path = r\"C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\Brawl_test\\BrawlTest_55.mp4\"\n",
    "fighting_count = count_fighting_people_in_video(video_path)\n",
    "print(\"Estimated maximum number of people involved in the brawl (smoothed):\", fighting_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This code processes an input video to detect and annotate people using YOLOv3. It loads the YOLOv3 configuration, weights, and class names from specified file paths. It extracts the output layers required for inference."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af958864165ef002"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VideoWriter object: < cv2.VideoWriter 00000279C8DFBC50>\n",
      "Processing complete. Output saved to: BonusTaskSample_BrawlTest55.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "def load_yolo_model(cfg_path, weights_path, names_path):\n",
    "    # Load YOLOv3 network\n",
    "    net = cv2.dnn.readNetFromDarknet(cfg_path, weights_path)\n",
    "    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "    # Get the output layer names\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers().flatten()]\n",
    "\n",
    "    # Load class names\n",
    "    with open(names_path, \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    return net, output_layers, classes\n",
    "\n",
    "def compute_iou(boxA, boxB):\n",
    "    \"\"\"Compute Intersection over Union (IoU) between two bounding boxes.\"\"\"\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
    "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = boxA[2] * boxA[3]\n",
    "    boxBArea = boxB[2] * boxB[3]\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
    "    return iou\n",
    "\n",
    "def detect_fight_in_frame(boxes, distance_threshold=60, iou_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Given a list of bounding boxes, return a set of indices for persons who are flagged\n",
    "    as potentially fighting. A person is flagged if the distance between centers is less than\n",
    "    distance_threshold or if the bounding boxes have significant overlap.\n",
    "    \"\"\"\n",
    "    fight_indices = set()\n",
    "    centers = []\n",
    "    \n",
    "    for (x, y, w, h) in boxes:\n",
    "        center = (x + w/2, y + h/2)\n",
    "        centers.append(center)\n",
    "    \n",
    "    for i in range(len(boxes)):\n",
    "        for j in range(i + 1, len(boxes)):\n",
    "            # Calculate Euclidean distance between centers\n",
    "            dist = np.linalg.norm(np.array(centers[i]) - np.array(centers[j]))\n",
    "            # Calculate IoU between bounding boxes\n",
    "            iou = compute_iou(boxes[i], boxes[j])\n",
    "            # Flag if they are close or overlapping\n",
    "            if dist < distance_threshold or iou > iou_threshold:\n",
    "                fight_indices.add(i)\n",
    "                fight_indices.add(j)\n",
    "                \n",
    "    return fight_indices\n",
    "\n",
    "def process_video(input_video_path, output_video_path, conf_threshold=0.5, nms_threshold=0.4,\n",
    "                  distance_threshold=60, iou_threshold=0.1, smoothing_window=5):\n",
    "    # Update paths accordingly\n",
    "    cfg_path = r\"C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\darknet\\cfg\\yolov3.cfg\"\n",
    "    weights_path = r\"C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\yolov3.weights\"\n",
    "    names_path = r\"C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\darknet\\data\\coco.names\"\n",
    "\n",
    "    # Check if YOLO files exist\n",
    "    for path in [cfg_path, weights_path, names_path]:\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    net, output_layers, classes = load_yolo_model(cfg_path, weights_path, names_path)\n",
    "\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise Exception(\"Error opening video file.\")\n",
    "\n",
    "    # Get video properties and create VideoWriter\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "    print(\"VideoWriter object:\", writer)\n",
    "\n",
    "    # Queue for smoothing fighting count over a few frames\n",
    "    fight_count_queue = deque(maxlen=smoothing_window)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Create blob and forward pass through the network\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        outs = net.forward(output_layers)\n",
    "\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        class_ids = []\n",
    "\n",
    "        # Process detections\n",
    "        for output in outs:\n",
    "            for detection in output:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if confidence > conf_threshold and classes[class_id] == \"person\":\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        # Apply Non-Maximum Suppression\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "        final_boxes = [boxes[i] for i in indices.flatten()] if indices is not None and len(indices) > 0 else []\n",
    "\n",
    "        # Determine fighting persons using heuristics\n",
    "        fighting_indices = detect_fight_in_frame(final_boxes, distance_threshold, iou_threshold)\n",
    "\n",
    "        total_persons = len(final_boxes)\n",
    "        fighting_persons = len(fighting_indices)\n",
    "\n",
    "        # Add the current fighting count to the queue for smoothing\n",
    "        fight_count_queue.append(fighting_persons)\n",
    "        # Compute the median count over the smoothing window\n",
    "        smoothed_fight_count = int(np.median(list(fight_count_queue)))\n",
    "\n",
    "        # Draw bounding boxes and annotate each person\n",
    "        for idx, box in enumerate(final_boxes):\n",
    "            x, y, w, h = box\n",
    "            if idx in fighting_indices:\n",
    "                color = (0, 0, 255)  # Red for potential fighters\n",
    "                label = f\"Fighter: {confidences[idx]:.2f}\"\n",
    "            else:\n",
    "                color = (0, 255, 0)  # Green for bystanders\n",
    "                label = f\"Person: {confidences[idx]:.2f}\"\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        # Draw counts on the frame (smoothed fighting count)\n",
    "        cv2.putText(frame, f\"Total Persons: {total_persons}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "        cv2.putText(frame, f\"Fighting Persons: {smoothed_fight_count}\", (10, 70),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # Write the processed frame\n",
    "        writer.write(frame)\n",
    "\n",
    "        # Optionally display the frame (for debugging)\n",
    "        # cv2.imshow(\"Detection\", frame)\n",
    "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        #     break\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    # cv2.destroyAllWindows()\n",
    "    print(\"Processing complete. Output saved to:\", output_video_path)\n",
    "\n",
    "# Example usage:\n",
    "input_video = r\"C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\Brawl_test\\BrawlTest_55.mp4\"\n",
    "output_video = r\"BonusTaskSample_BrawlTest55.mp4\"\n",
    "process_video(input_video, output_video)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T16:02:22.631860Z",
     "start_time": "2025-03-25T16:01:22.807359Z"
    }
   },
   "id": "7eb67385bd72a908",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": [
    "!jupyter nbconvert --to notebook --inplace Brawl_Detector.ipynb\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-31T10:05:29.489409Z",
     "start_time": "2025-03-31T10:05:27.464159Z"
    }
   },
   "id": "19061f53814accb0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Brawl_Detector.ipynb to notebook\n",
      "[NbConvertApp] Writing 64081 bytes to Brawl_Detector.ipynb\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4c357439c14131"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
