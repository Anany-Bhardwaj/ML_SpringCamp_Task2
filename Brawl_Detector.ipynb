{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5623420dfc216d39",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "                                OVERALL AIM OF THE PROJECT\n",
    "The overall model aims to analyze video data by leveraging spatiotemporal features extracted using the I3D model, and then classifying the videos based on those features into categories such as \"peace\" or \"brawl.\" Additionally, by integrating YOLOv3-based people detection, the system can provide further insights—such as counting the number of people present—which might be indicative of the intensity of a brawl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b440b39670172605",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The code imports necessary libraries (OpenCV, NumPy, TensorFlow, and TensorFlow Hub) and sets up some parameters like the dataset path, video categories, and input size/frame count expected by the I3D model. It loads the I3D (Inflated 3D ConvNet) model from TensorFlow Hub. This model is pre-trained on the Kinetics-400 dataset and is used for extracting spatiotemporal features from videos.\n",
    "\n",
    "The code iterates over two video categories (\"peace\" and \"brawl\"). For each video, it extracts frames using the helper function, normalizes them, and passes them through the I3D model to extract features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T11:34:20.899331Z",
     "start_time": "2025-03-25T11:17:44.649346Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 800 videos in category 'peace'...\n",
      "Processing 800 videos in category 'brawl'...\n",
      "Extracted features shape: (1600, 400)\n",
      "Labels shape: (1600,)\n",
      "I3D feature extraction complete and data saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Parameters\n",
    "DATASET_PATH = r\"C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Dataset\"\n",
    "CATEGORIES = [\"peace\", \"brawl\"]   # Folder names; 0: peace, 1: brawl\n",
    "NUM_I3D_FRAMES = 64               # I3D expects 64 frames per video\n",
    "FRAME_SIZE = (224, 224)           # I3D default input size\n",
    "\n",
    "# Explicitly set signature to \"default\" so that we expect the default behavior.\n",
    "i3d_url = \"https://tfhub.dev/deepmind/i3d-kinetics-400/1\"\n",
    "i3d_layer = hub.KerasLayer(i3d_url, trainable=False, signature=\"default\")\n",
    "\n",
    "def load_video_frames_i3d(video_path, num_frames=NUM_I3D_FRAMES, size=FRAME_SIZE):\n",
    "    \"\"\"\n",
    "    Loads a video, samples or pads to `num_frames` frames, resizes each frame to `size`,\n",
    "    and returns an array of frames.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total_frames <= 0:\n",
    "        cap.release()\n",
    "        return None\n",
    "\n",
    "    # Uniform sampling indices (if video has fewer frames, use all available indices)\n",
    "    if total_frames >= num_frames:\n",
    "        frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    else:\n",
    "        frame_indices = np.arange(total_frames)\n",
    "\n",
    "    frames = []\n",
    "    frame_id = 0\n",
    "    ret = True\n",
    "    while ret:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_id in frame_indices:\n",
    "            frame = cv2.resize(frame, size)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "        frame_id += 1\n",
    "    cap.release()\n",
    "\n",
    "    # Pad with the last frame if fewer than num_frames\n",
    "    if len(frames) < num_frames:\n",
    "        last_frame = frames[-1] if frames else np.zeros((size[1], size[0], 3), dtype=np.uint8)\n",
    "        for _ in range(num_frames - len(frames)):\n",
    "            frames.append(last_frame)\n",
    "    \n",
    "    return np.array(frames)\n",
    "\n",
    "# Lists to store extracted features and corresponding labels\n",
    "video_features = []\n",
    "video_labels = []\n",
    "\n",
    "for label, category in enumerate(CATEGORIES):\n",
    "    folder_path = os.path.join(DATASET_PATH, category)\n",
    "    video_files = glob.glob(os.path.join(folder_path, \"*.mp4\"))  # Adjust extension if needed\n",
    "    print(f\"Processing {len(video_files)} videos in category '{category}'...\")\n",
    "    for video_path in video_files:\n",
    "        frames = load_video_frames_i3d(video_path)\n",
    "        if frames is not None:\n",
    "            # Normalize frames to [0,1]\n",
    "            frames = frames.astype('float32') / 255.0\n",
    "            # Expand dims to create a batch of size 1: shape becomes (1, 64, 224, 224, 3)\n",
    "            video_tensor = np.expand_dims(frames, axis=0)\n",
    "            # Extract features using the I3D model (returns a tensor directly)\n",
    "            features_tensor = i3d_layer(video_tensor)\n",
    "            # Convert tensor to numpy array and remove batch dimension\n",
    "            features = np.squeeze(features_tensor.numpy())\n",
    "            video_features.append(features)\n",
    "            video_labels.append(label)\n",
    "        else:\n",
    "            print(f\"Warning: Could not load video: {video_path}\")\n",
    "\n",
    "video_features = np.array(video_features)\n",
    "video_labels = np.array(video_labels)\n",
    "\n",
    "print(\"Extracted features shape:\", video_features.shape)\n",
    "print(\"Labels shape:\", video_labels.shape)\n",
    "\n",
    "# Save the features and labels for later training\n",
    "np.save(\"i3d_video_features.npy\", video_features)\n",
    "np.save(\"i3d_video_labels.npy\", video_labels)\n",
    "print(\"I3D feature extraction complete and data saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d2f9fcd73be75c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This code loads pre-extracted I3D video features and their corresponding labels, then builds, trains, and evaluates a simple Multi-Layer Perceptron (MLP) classifier to distinguish between two classes (\"peace\" vs. \"brawl\"). It uses an 80/20 train-test split-ratio , employs callbacks for early stopping and model checkpointing, and finally computes evaluation metrics such as accuracy, F1 score, confusion matrix, and a detailed classification report before saving the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbb171783fb55a58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T10:08:27.159535Z",
     "start_time": "2025-03-24T10:08:21.824033Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (1600, 400)\n",
      "Labels shape: (1600,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nitro 5\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">102,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m102,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">135,681</span> (530.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m135,681\u001b[0m (530.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">135,681</span> (530.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m135,681\u001b[0m (530.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m35/72\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5698 - loss: 2.1089   \n",
      "Epoch 1: val_loss improved from inf to 0.40528, saving model to best_i3d_classifier.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5877 - loss: 1.9433 - val_accuracy: 0.8359 - val_loss: 0.4053\n",
      "Epoch 2/50\n",
      "\u001b[1m70/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7469 - loss: 1.0096\n",
      "Epoch 2: val_loss improved from 0.40528 to 0.28674, saving model to best_i3d_classifier.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7469 - loss: 1.0068 - val_accuracy: 0.8516 - val_loss: 0.2867\n",
      "Epoch 3/50\n",
      "\u001b[1m67/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7710 - loss: 0.8057\n",
      "Epoch 3: val_loss improved from 0.28674 to 0.22963, saving model to best_i3d_classifier.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7720 - loss: 0.8003 - val_accuracy: 0.8828 - val_loss: 0.2296\n",
      "Epoch 4/50\n",
      "\u001b[1m65/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7844 - loss: 0.6462\n",
      "Epoch 4: val_loss improved from 0.22963 to 0.19343, saving model to best_i3d_classifier.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7856 - loss: 0.6457 - val_accuracy: 0.9219 - val_loss: 0.1934\n",
      "Epoch 5/50\n",
      "\u001b[1m68/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8256 - loss: 0.5442\n",
      "Epoch 5: val_loss improved from 0.19343 to 0.16390, saving model to best_i3d_classifier.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8264 - loss: 0.5411 - val_accuracy: 0.9375 - val_loss: 0.1639\n",
      "Epoch 6/50\n",
      "\u001b[1m38/72\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8828 - loss: 0.3527 \n",
      "Epoch 6: val_loss improved from 0.16390 to 0.14739, saving model to best_i3d_classifier.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8677 - loss: 0.3952 - val_accuracy: 0.9453 - val_loss: 0.1474\n",
      "Epoch 7/50\n",
      "\u001b[1m39/72\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8699 - loss: 0.3398 \n",
      "Epoch 7: val_loss improved from 0.14739 to 0.13170, saving model to best_i3d_classifier.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8670 - loss: 0.3454 - val_accuracy: 0.9453 - val_loss: 0.1317\n",
      "Epoch 8/50\n",
      "\u001b[1m38/72\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8975 - loss: 0.3213 \n",
      "Epoch 8: val_loss improved from 0.13170 to 0.12063, saving model to best_i3d_classifier.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8876 - loss: 0.3315 - val_accuracy: 0.9453 - val_loss: 0.1206\n",
      "Epoch 9/50\n",
      "\u001b[1m38/72\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8925 - loss: 0.2710 \n",
      "Epoch 9: val_loss improved from 0.12063 to 0.11351, saving model to best_i3d_classifier.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8908 - loss: 0.2803 - val_accuracy: 0.9453 - val_loss: 0.1135\n",
      "Epoch 10/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8885 - loss: 0.3333\n",
      "Epoch 10: val_loss improved from 0.11351 to 0.10677, saving model to best_i3d_classifier.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8886 - loss: 0.3325 - val_accuracy: 0.9453 - val_loss: 0.1068\n",
      "Epoch 11/50\n",
      "\u001b[1m39/72\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9181 - loss: 0.2107 \n",
      "Epoch 11: val_loss improved from 0.10677 to 0.10520, saving model to best_i3d_classifier.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9146 - loss: 0.2299 - val_accuracy: 0.9531 - val_loss: 0.1052\n",
      "Epoch 12/50\n",
      "\u001b[1m39/72\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9062 - loss: 0.2374 \n",
      "Epoch 12: val_loss did not improve from 0.10520\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9093 - loss: 0.2367 - val_accuracy: 0.9531 - val_loss: 0.1077\n",
      "Epoch 13/50\n",
      "\u001b[1m41/72\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8991 - loss: 0.2790 \n",
      "Epoch 13: val_loss improved from 0.10520 to 0.10090, saving model to best_i3d_classifier.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9066 - loss: 0.2593 - val_accuracy: 0.9531 - val_loss: 0.1009\n",
      "Epoch 14/50\n",
      "\u001b[1m40/72\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9119 - loss: 0.2070 \n",
      "Epoch 14: val_loss did not improve from 0.10090\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9129 - loss: 0.2105 - val_accuracy: 0.9531 - val_loss: 0.1032\n",
      "Epoch 15/50\n",
      "\u001b[1m42/72\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9049 - loss: 0.2638 \n",
      "Epoch 15: val_loss improved from 0.10090 to 0.09716, saving model to best_i3d_classifier.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9140 - loss: 0.2365 - val_accuracy: 0.9531 - val_loss: 0.0972\n",
      "Epoch 16/50\n",
      "\u001b[1m41/72\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9226 - loss: 0.2184 \n",
      "Epoch 16: val_loss improved from 0.09716 to 0.09582, saving model to best_i3d_classifier.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9273 - loss: 0.2003 - val_accuracy: 0.9609 - val_loss: 0.0958\n",
      "Epoch 17/50\n",
      "\u001b[1m39/72\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9196 - loss: 0.2191 \n",
      "Epoch 17: val_loss improved from 0.09582 to 0.08521, saving model to best_i3d_classifier.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9236 - loss: 0.2027 - val_accuracy: 0.9688 - val_loss: 0.0852\n",
      "Epoch 18/50\n",
      "\u001b[1m36/72\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9412 - loss: 0.1478 \n",
      "Epoch 18: val_loss did not improve from 0.08521\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9374 - loss: 0.1492 - val_accuracy: 0.9609 - val_loss: 0.0869\n",
      "Epoch 19/50\n",
      "\u001b[1m37/72\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9437 - loss: 0.1957 \n",
      "Epoch 19: val_loss did not improve from 0.08521\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9422 - loss: 0.1845 - val_accuracy: 0.9688 - val_loss: 0.0852\n",
      "Epoch 20/50\n",
      "\u001b[1m42/72\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9625 - loss: 0.0878 \n",
      "Epoch 20: val_loss did not improve from 0.08521\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9571 - loss: 0.1029 - val_accuracy: 0.9609 - val_loss: 0.0911\n",
      "Epoch 21/50\n",
      "\u001b[1m41/72\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9523 - loss: 0.1777 \n",
      "Epoch 21: val_loss did not improve from 0.08521\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9485 - loss: 0.1648 - val_accuracy: 0.9609 - val_loss: 0.0854\n",
      "Epoch 22/50\n",
      "\u001b[1m39/72\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9424 - loss: 0.1176 \n",
      "Epoch 22: val_loss did not improve from 0.08521\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9451 - loss: 0.1202 - val_accuracy: 0.9609 - val_loss: 0.0882\n",
      "Epoch 22: early stopping\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9368 - loss: 0.1610 \n",
      "Test Loss: 0.1663, Test Accuracy: 0.9312\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9345\n",
      "Confusion Matrix:\n",
      "[[141  19]\n",
      " [  3 157]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.88      0.93       160\n",
      "           1       0.89      0.98      0.93       160\n",
      "\n",
      "    accuracy                           0.93       320\n",
      "   macro avg       0.94      0.93      0.93       320\n",
      "weighted avg       0.94      0.93      0.93       320\n",
      "\n",
      "Model training complete and model saved.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the extracted I3D features and labels\n",
    "features = np.load(\"i3d_video_features.npy\")  # shape: (num_videos, feature_dim)\n",
    "labels = np.load(\"i3d_video_labels.npy\")        # shape: (num_videos,)\n",
    "\n",
    "print(\"Features shape:\", features.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "# Split data into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Build a simple MLP classifier on top of the I3D features\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(features.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification (peace:0, brawl:1)\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and binary crossentropy loss\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Define callbacks for early stopping and saving the best model\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint(\"best_i3d_classifier.h5\", monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "# Train the classifier\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the classifier on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate predictions on the test set\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate F1 score and print evaluation metrics\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the final trained model\n",
    "model.save(\"final_i3d_classifier.h5\")\n",
    "print(\"Model training complete and model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9bc59e6eea10740",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T13:29:10.197321Z",
     "start_time": "2025-03-24T13:29:09.648180Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete and model saved.\n"
     ]
    }
   ],
   "source": [
    "model.save(\"final_i3d_classifier.keras\")\n",
    "print(\"Model training complete and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23b51c441e0f2a32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T14:08:08.355371Z",
     "start_time": "2025-03-24T14:08:00.767656Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nitro 5\\venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 8 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Load trained classifier model\n",
    "model = tf.keras.models.load_model(\"final_i3d_classifier.keras\")\n",
    "\n",
    "# Load I3D feature extractor\n",
    "i3d_url = \"https://tfhub.dev/deepmind/i3d-kinetics-400/1\"\n",
    "i3d_layer = hub.KerasLayer(i3d_url, trainable=False, signature=\"default\")\n",
    "\n",
    "# Parameters\n",
    "TEST_DATASET_PATH = r\"C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\"\n",
    "CATEGORIES = [\"peace_test\", \"brawl_test\"]  # 0: peace, 1: brawl\n",
    "NUM_I3D_FRAMES = 64  # I3D expects 64 frames\n",
    "FRAME_SIZE = (224, 224)  # I3D default input size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d365fa3d2b228ba7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T14:09:18.123105Z",
     "start_time": "2025-03-24T14:09:18.116576Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_video_frames_i3d(video_path, num_frames=NUM_I3D_FRAMES, size=FRAME_SIZE):\n",
    "    \"\"\"\n",
    "    Loads a video, samples/pads it to `num_frames`, resizes frames, and returns a NumPy array.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total_frames <= 0:\n",
    "        cap.release()\n",
    "        return None\n",
    "\n",
    "    # Sample frames uniformly or use all frames if fewer than num_frames\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int) if total_frames >= num_frames else np.arange(total_frames)\n",
    "\n",
    "    frames = []\n",
    "    frame_id = 0\n",
    "    ret = True\n",
    "    while ret:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_id in frame_indices:\n",
    "            frame = cv2.resize(frame, size)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "        frame_id += 1\n",
    "    cap.release()\n",
    "\n",
    "    # Pad with last frame if too few\n",
    "    if len(frames) < num_frames:\n",
    "        last_frame = frames[-1] if frames else np.zeros((size[1], size[0], 3), dtype=np.uint8)\n",
    "        for _ in range(num_frames - len(frames)):\n",
    "            frames.append(last_frame)\n",
    "    \n",
    "    return np.array(frames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d4b9cff74c8dc849",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T14:17:12.460690Z",
     "start_time": "2025-03-24T14:09:30.458839Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 200 videos in 'peace_test'...\n",
      "Processing 200 videos in 'brawl_test'...\n",
      "Test features shape: (400, 400)\n",
      "Test labels shape: (400,)\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# Lists to store predictions and actual labels\n",
    "test_features = []\n",
    "test_labels = []\n",
    "video_paths = []\n",
    "\n",
    "for label, category in enumerate(CATEGORIES):\n",
    "    folder_path = os.path.join(TEST_DATASET_PATH, category)\n",
    "    video_files = glob.glob(os.path.join(folder_path, \"*.mp4\"))  # Change extension if needed\n",
    "    print(f\"Processing {len(video_files)} videos in '{category}'...\")\n",
    "\n",
    "    for video_path in video_files:\n",
    "        frames = load_video_frames_i3d(video_path)\n",
    "        if frames is not None:\n",
    "            # Normalize frames to [0,1]\n",
    "            frames = frames.astype('float32') / 255.0\n",
    "            # Expand dims to create a batch of size 1: (1, 64, 224, 224, 3)\n",
    "            video_tensor = np.expand_dims(frames, axis=0)\n",
    "            # Extract I3D features\n",
    "            features_tensor = i3d_layer(video_tensor)\n",
    "            # Convert to numpy array and remove batch dimension\n",
    "            features = np.squeeze(features_tensor.numpy())\n",
    "            test_features.append(features)\n",
    "            test_labels.append(label)\n",
    "            video_paths.append(video_path)\n",
    "        else:\n",
    "            print(f\"Warning: Could not load video: {video_path}\")\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "test_features = np.array(test_features)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(\"Test features shape:\", test_features.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)\n",
    "\n",
    "# Make predictions\n",
    "pred_probs = model.predict(test_features)\n",
    "pred_labels = (pred_probs > 0.5).astype(int).flatten()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3679af4b1da4e731",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T17:54:29.801912Z",
     "start_time": "2025-03-25T17:54:29.304696Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[180  20]\n",
      " [  2 198]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       peace       0.99      0.90      0.94       200\n",
      "       brawl       0.91      0.99      0.95       200\n",
      "\n",
      "    accuracy                           0.94       400\n",
      "   macro avg       0.95      0.95      0.94       400\n",
      "weighted avg       0.95      0.94      0.94       400\n",
      "\n",
      "\n",
      "F1 Score: 0.9474\n",
      "\n",
      "Predictions saved to predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "# Print classification metrics\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_labels, pred_labels))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, pred_labels, target_names=[\"peace\", \"brawl\"]))\n",
    "\n",
    "# Calculate and print F1 score\n",
    "f1 = f1_score(test_labels, pred_labels)\n",
    "print(f\"\\nF1 Score: {f1:.4f}\")\n",
    "\n",
    "# Generate video IDs (e.g., Brawl_Test1, Peace_Test2)\n",
    "video_ids = []\n",
    "category_counts = {\"peace\": 0, \"brawl\": 1}\n",
    "\n",
    "for i, path in enumerate(video_paths):\n",
    "    category = \"brawl\" if test_labels[i] == 1 else \"peace\"\n",
    "    video_id = f\"{category.capitalize()}_Test{category_counts[category]}\"\n",
    "    video_ids.append(video_id)\n",
    "    category_counts[category] += 1\n",
    "\n",
    "# Create DataFrame and save predictions\n",
    "df = pd.DataFrame({\"videoID\": video_ids, \"prediction\": [\"peace\" if p == 0 else \"brawl\" for p in pred_labels]})\n",
    "df.to_csv(\"predictions.csv\", index=False)\n",
    "\n",
    "print(\"\\nPredictions saved to predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c84a91a1b64f17ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T11:17:28.351182Z",
     "start_time": "2025-03-25T11:17:28.345693Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Misclassified videos (22):\n",
      "Actual: peace, Predicted: brawl, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\peace_test\\PeaceTest_105.mp4\n",
      "Actual: peace, Predicted: brawl, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\peace_test\\PeaceTest_106.mp4\n",
      "Actual: peace, Predicted: brawl, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\peace_test\\PeaceTest_107.mp4\n",
      "Actual: peace, Predicted: brawl, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\peace_test\\PeaceTest_108.mp4\n",
      "Actual: peace, Predicted: brawl, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\peace_test\\PeaceTest_109.mp4\n",
      "Actual: peace, Predicted: brawl, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\peace_test\\PeaceTest_110.mp4\n",
      "Actual: peace, Predicted: brawl, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\peace_test\\PeaceTest_111.mp4\n",
      "Actual: peace, Predicted: brawl, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\peace_test\\PeaceTest_122.mp4\n",
      "Actual: peace, Predicted: brawl, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\peace_test\\PeaceTest_131.mp4\n",
      "Actual: peace, Predicted: brawl, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\peace_test\\PeaceTest_142.mp4\n",
      "Actual: peace, Predicted: brawl, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\peace_test\\PeaceTest_143.mp4\n",
      "Actual: peace, Predicted: brawl, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\peace_test\\PeaceTest_166.mp4\n",
      "Actual: peace, Predicted: brawl, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\peace_test\\PeaceTest_168.mp4\n",
      "Actual: peace, Predicted: brawl, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\peace_test\\PeaceTest_172.mp4\n",
      "Actual: peace, Predicted: brawl, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\peace_test\\PeaceTest_174.mp4\n",
      "Actual: peace, Predicted: brawl, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\peace_test\\PeaceTest_187.mp4\n",
      "Actual: peace, Predicted: brawl, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\peace_test\\PeaceTest_188.mp4\n",
      "Actual: peace, Predicted: brawl, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\peace_test\\PeaceTest_189.mp4\n",
      "Actual: peace, Predicted: brawl, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\peace_test\\PeaceTest_6.mp4\n",
      "Actual: peace, Predicted: brawl, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\peace_test\\PeaceTest_7.mp4\n",
      "Actual: brawl, Predicted: peace, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\brawl_test\\BrawlTest_167.mp4\n",
      "Actual: brawl, Predicted: peace, File: C:\\Users\\Nitro 5\\PycharmProjects\\pythonProject1\\Test\\brawl_test\\BrawlTest_47.mp4\n"
     ]
    }
   ],
   "source": [
    "misclassified = np.where(test_labels != pred_labels)[0]\n",
    "print(f\"\\nMisclassified videos ({len(misclassified)}):\")\n",
    "for i in misclassified:\n",
    "    print(f\"Actual: {CATEGORIES[test_labels[i]]}, Predicted: {CATEGORIES[pred_labels[i]]}, File: {video_paths[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8306ce2fba16a5a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "509cb7e5e68aebac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T21:56:55.652266Z",
     "start_time": "2025-03-26T21:56:51.538480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbformat in c:\\users\\nitro 5\\venv\\lib\\site-packages (5.9.2)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\nitro 5\\venv\\lib\\site-packages (from nbformat) (2.19.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\nitro 5\\venv\\lib\\site-packages (from nbformat) (4.20.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\nitro 5\\venv\\lib\\site-packages (from nbformat) (5.7.0)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\nitro 5\\venv\\lib\\site-packages (from nbformat) (5.14.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\nitro 5\\venv\\lib\\site-packages (from jsonschema>=2.6->nbformat) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\nitro 5\\venv\\lib\\site-packages (from jsonschema>=2.6->nbformat) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\nitro 5\\venv\\lib\\site-packages (from jsonschema>=2.6->nbformat) (0.32.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\nitro 5\\venv\\lib\\site-packages (from jsonschema>=2.6->nbformat) (0.16.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\nitro 5\\venv\\lib\\site-packages (from jupyter-core->nbformat) (4.1.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\nitro 5\\venv\\lib\\site-packages (from jupyter-core->nbformat) (306)\n"
     ]
    }
   ],
   "source": [
    "!pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eee17507a7dbc1f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T21:57:25.143429Z",
     "start_time": "2025-03-26T21:57:24.508036Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nitro 5\\venv\\Scripts\\python.exe: No module named nbformat.__main__; 'nbformat' is a package and cannot be directly executed\n"
     ]
    }
   ],
   "source": [
    "!python -m nbformat --validate Brawl_Detector.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38ad9cbf7fea396e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T21:58:51.257155Z",
     "start_time": "2025-03-26T21:58:48.878396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This application is used to convert notebook files (*.ipynb)\n",
      "        to various other formats.\n",
      "\n",
      "        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
      "\n",
      "Options\n",
      "=======\n",
      "The options below are convenience aliases to configurable class-options,\n",
      "as listed in the \"Equivalent to\" description-line of the aliases.\n",
      "To see all configurable class-options for some <cmd>, use:\n",
      "    <cmd> --help-all\n",
      "\n",
      "--debug\n",
      "    set log level to logging.DEBUG (maximize logging output)\n",
      "    Equivalent to: [--Application.log_level=10]\n",
      "--show-config\n",
      "    Show the application's configuration (human-readable format)\n",
      "    Equivalent to: [--Application.show_config=True]\n",
      "--show-config-json\n",
      "    Show the application's configuration (json format)\n",
      "    Equivalent to: [--Application.show_config_json=True]\n",
      "--generate-config\n",
      "    generate default config file\n",
      "    Equivalent to: [--JupyterApp.generate_config=True]\n",
      "-y\n",
      "    Answer yes to any questions instead of prompting.\n",
      "    Equivalent to: [--JupyterApp.answer_yes=True]\n",
      "--execute\n",
      "    Execute the notebook prior to export.\n",
      "    Equivalent to: [--ExecutePreprocessor.enabled=True]\n",
      "--allow-errors\n",
      "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
      "    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n",
      "--stdin\n",
      "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
      "    Equivalent to: [--NbConvertApp.from_stdin=True]\n",
      "--stdout\n",
      "    Write notebook output to stdout instead of files.\n",
      "    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n",
      "--inplace\n",
      "    Run nbconvert in place, overwriting the existing notebook (only\n",
      "            relevant when converting to notebook format)\n",
      "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n",
      "--clear-output\n",
      "    Clear output of current file and save in place,\n",
      "            overwriting the existing notebook.\n",
      "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n",
      "--coalesce-streams\n",
      "    Coalesce consecutive stdout and stderr outputs into one stream (within each cell).\n",
      "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --CoalesceStreamsPreprocessor.enabled=True]\n",
      "--no-prompt\n",
      "    Exclude input and output prompts from converted document.\n",
      "    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n",
      "--no-input\n",
      "    Exclude input cells and output prompts from converted document.\n",
      "            This mode is ideal for generating code-free reports.\n",
      "    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\n",
      "--allow-chromium-download\n",
      "    Whether to allow downloading chromium if no suitable version is found on the system.\n",
      "    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\n",
      "--disable-chromium-sandbox\n",
      "    Disable chromium security sandbox when converting to PDF..\n",
      "    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\n",
      "--show-input\n",
      "    Shows code input. This flag is only useful for dejavu users.\n",
      "    Equivalent to: [--TemplateExporter.exclude_input=False]\n",
      "--embed-images\n",
      "    Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.\n",
      "    Equivalent to: [--HTMLExporter.embed_images=True]\n",
      "--sanitize-html\n",
      "    Whether the HTML in Markdown cells and cell outputs should be sanitized..\n",
      "    Equivalent to: [--HTMLExporter.sanitize_html=True]\n",
      "--log-level=<Enum>\n",
      "    Set the log level by value or name.\n",
      "    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n",
      "    Default: 30\n",
      "    Equivalent to: [--Application.log_level]\n",
      "--config=<Unicode>\n",
      "    Full path of a config file.\n",
      "    Default: ''\n",
      "    Equivalent to: [--JupyterApp.config_file]\n",
      "--to=<Unicode>\n",
      "    The export format to be used, either one of the built-in formats\n",
      "            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf']\n",
      "            or a dotted object name that represents the import path for an\n",
      "            ``Exporter`` class\n",
      "    Default: ''\n",
      "    Equivalent to: [--NbConvertApp.export_format]\n",
      "--template=<Unicode>\n",
      "    Name of the template to use\n",
      "    Default: ''\n",
      "    Equivalent to: [--TemplateExporter.template_name]\n",
      "--template-file=<Unicode>\n",
      "    Name of the template file to use\n",
      "    Default: None\n",
      "    Equivalent to: [--TemplateExporter.template_file]\n",
      "--theme=<Unicode>\n",
      "    Template specific theme(e.g. the name of a JupyterLab CSS theme distributed\n",
      "    as prebuilt extension for the lab template)\n",
      "    Default: 'light'\n",
      "    Equivalent to: [--HTMLExporter.theme]\n",
      "--sanitize_html=<Bool>\n",
      "    Whether the HTML in Markdown cells and cell outputs should be sanitized.This\n",
      "    should be set to True by nbviewer or similar tools.\n",
      "    Default: False\n",
      "    Equivalent to: [--HTMLExporter.sanitize_html]\n",
      "--writer=<DottedObjectName>\n",
      "    Writer class used to write the\n",
      "                                        results of the conversion\n",
      "    Default: 'FilesWriter'\n",
      "    Equivalent to: [--NbConvertApp.writer_class]\n",
      "--post=<DottedOrNone>\n",
      "    PostProcessor class used to write the\n",
      "                                        results of the conversion\n",
      "    Default: ''\n",
      "    Equivalent to: [--NbConvertApp.postprocessor_class]\n",
      "--output=<Unicode>\n",
      "    Overwrite base name use for output files.\n",
      "                Supports pattern replacements '{notebook_name}'.\n",
      "    Default: '{notebook_name}'\n",
      "    Equivalent to: [--NbConvertApp.output_base]\n",
      "--output-dir=<Unicode>\n",
      "    Directory to write output(s) to. Defaults\n",
      "                                  to output to the directory of each notebook. To recover\n",
      "                                  previous default behaviour (outputting to the current\n",
      "                                  working directory) use . as the flag value.\n",
      "    Default: ''\n",
      "    Equivalent to: [--FilesWriter.build_directory]\n",
      "--reveal-prefix=<Unicode>\n",
      "    The URL prefix for reveal.js (version 3.x).\n",
      "            This defaults to the reveal CDN, but can be any url pointing to a copy\n",
      "            of reveal.js.\n",
      "            For speaker notes to work, this must be a relative path to a local\n",
      "            copy of reveal.js: e.g., \"reveal.js\".\n",
      "            If a relative path is given, it must be a subdirectory of the\n",
      "            current directory (from which the server is run).\n",
      "            See the usage documentation\n",
      "            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n",
      "            for more details.\n",
      "    Default: ''\n",
      "    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n",
      "--nbformat=<Enum>\n",
      "    The nbformat version to write.\n",
      "            Use this to downgrade notebooks.\n",
      "    Choices: any of [1, 2, 3, 4]\n",
      "    Default: 4\n",
      "    Equivalent to: [--NotebookExporter.nbformat_version]\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "    The simplest way to use nbconvert is\n",
      "\n",
      "            > jupyter nbconvert mynotebook.ipynb --to html\n",
      "\n",
      "            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf'].\n",
      "\n",
      "            > jupyter nbconvert --to latex mynotebook.ipynb\n",
      "\n",
      "            Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
      "            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and\n",
      "            'classic'. You can specify the flavor of the format used.\n",
      "\n",
      "            > jupyter nbconvert --to html --template lab mynotebook.ipynb\n",
      "\n",
      "            You can also pipe the output to stdout, rather than a file\n",
      "\n",
      "            > jupyter nbconvert mynotebook.ipynb --stdout\n",
      "\n",
      "            PDF is generated via latex\n",
      "\n",
      "            > jupyter nbconvert mynotebook.ipynb --to pdf\n",
      "\n",
      "            You can get (and serve) a Reveal.js-powered slideshow\n",
      "\n",
      "            > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
      "\n",
      "            Multiple notebooks can be given at the command line in a couple of\n",
      "            different ways:\n",
      "\n",
      "            > jupyter nbconvert notebook*.ipynb\n",
      "            > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
      "\n",
      "            or you can specify the notebooks list in a config file, containing::\n",
      "\n",
      "                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
      "\n",
      "            > jupyter nbconvert --config mycfg.py\n",
      "\n",
      "To see all available configurables, use `--help-all`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] WARNING | pattern 'your_notebook.ipynb' matched no files\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to notebook --inplace Brawl_Detector.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83574caf380266e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
